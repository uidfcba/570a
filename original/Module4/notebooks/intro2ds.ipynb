{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Descriptive Statistics\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic tasks in data analytics, particularly for large data sets, is describing a data set by a small number of statistical quantities. For example, we may wish to know the average income in a geographic region, or the general spread about this average income. In this notebook, we introduce statistical quantities that can be used to describe a (one-dimensional) data set and to enable comparisons between data sets. Fundamentally, this task includes computing\n",
    "\n",
    "1. a measure of location of centrality,\n",
    "2. a measure of variability or spread,\n",
    "3. measures of the distribution, and\n",
    "4. measures of the distribution shape.\n",
    "\n",
    "In addition, we also will look at measuring these quantities when the data have been measured with different precision (or have different errors) before finally concluding with a discussion on populations and samples.\n",
    "\n",
    "First, however, we include our standard imports and provide a brief exploration of a demo dataset before generating a NumPy array for our `total_bill` column to simplify computing descriptive statistics.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn.apionly as sns\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# Load the Tips Data\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Display first few rows\n",
    "tips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.27,  35.26,  15.42,  18.43,  14.83,  21.58,  10.33,  16.29,\n",
       "        16.97,  20.65])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the total_bill column to a NumPy array\n",
    "data = tips.total_bill.as_matrix()\n",
    "data[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Location (Centrality)\n",
    "\n",
    "### [Mean][1]\n",
    "\n",
    "Given a data set, for example, the `total_bill` column in the _tips_ data set, the first step is often to compute a typical value, in this case the typical bill. The standard approach to this task is to compute the average or (arithmetic) mean ($\\mu$ or $\\overline{x}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Mean} = \\mu = \\overline{x} = \\frac{\\sum_{i=1}^N x_i}{N}\n",
    "\\end{equation}\n",
    "\n",
    "Computing this in Python is generally quite simple, and most data structures (including both Pandas and NumPy) include helper functions for comptuing this and other descriptive statistics. For example, we can compute the mean value for the `total_bill` column in the tips data:\n",
    "\n",
    "```python\n",
    "tips[['total_bill']].mean()\n",
    "```\n",
    "\n",
    "In subsequent lessons, we will explore theoretical distributions, such as the Normal or Gaussian distribution, for which the arithmetic mean plays an important role. As a descriptive statistic, however, other means can also be computed, including the geometric and harmonic means:\n",
    "\n",
    "- Geometric mean: The n-th root of a set of values\n",
    "- Harmonic mean: The mean of ratio quantities\n",
    "\n",
    "More generally, we can use the NumPy module (on the `data` array) to [compute the arithmetic mean][p1] and other descriptive statistics, as demonstrated in the following code cell.\n",
    "\n",
    "-----\n",
    "[1]: https://en.wikipedia.org/wiki/Mean\n",
    "[p1]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 19.7859\n",
      "Geometric Mean Value = 17.9989\n",
      "Harmonic Mean Value = 16.3090\n"
     ]
    }
   ],
   "source": [
    "# We compute three different mean quantities\n",
    "print('Mean Value = {:6.4f}'.format(np.mean(data)))\n",
    "print('Geometric Mean Value = {:6.4f}'.format(sp.stats.gmean(data)))\n",
    "print('Harmonic Mean Value = {:6.4f}'.format(sp.stats.hmean(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The mean (or average) is a commonly used descriptive statistic. However, some data are distributed in ways that make the mean a poor measure for the typical value. For example, if data are bi-modal (with lots of low values and lots of high values) the mean will typically lie between these two clumps of data. Likewise, this can occur if a data set has a number of outliers at one end of a range (such as sales contracts that are much higher than the rest of the sales contracts in a list). In this case, other measures can provide more robust estimates. Some of the more common such measures include the median, the mode, and trimmed mean.\n",
    "\n",
    "\n",
    "### [Median][2]\n",
    "\n",
    "The median is conceptually an easy measure of centrality; half the data lie above and below the median. Thus, to compute the median you first sort the data and select the middle element. While conceptually simple, in practice computing the median is complicated by several issues:\n",
    "\n",
    "1. We need to sort the data, which is challenging for big data (and also implies the data can be sorted).\n",
    "2. For a data set with an even number of values there is no middle value.\n",
    "\n",
    "In the last case, for example, given the list of points `[0, 1, 2, 3, 4, 5]`, we can use three different techniques to compute a median:\n",
    "a. average the two middle values (`median=2.5`),\n",
    "b. take the lower of the two middle values (`median=2`), or\n",
    "c. take the higher of the two middle values (`median=3`).\n",
    "\n",
    "Often, a [Python module][p2] by default employs the first technique (average the two middle values), although, as demonstrated below, exceptions to this do occur, such as in the built-in Python `statistics` module, which can return any of these three values.\n",
    "\n",
    "-----\n",
    "\n",
    "### [Mode][3]\n",
    "\n",
    "The mode is simply the most common value in a data set. Often, a mode makes the most sense when the data have been binned, in which case the data have been aggregated and it becomes simple to find the bin with the most values (we will see this in a later lesson on distributions). A mode can also make sense when data are categorical, or explicitly limited to a discrete set of values. When [calculating the mode][p3] (or modal value), we also have the number of times this value occurred, as demonstrated below.\n",
    "\n",
    "-----\n",
    "\n",
    "### [Trimmed Mean][4]\n",
    "\n",
    "One simple technique to compute a more robust, average value is to trim outlier points before calculating the mean value, producing a _trimmed mean_. The trimming process generally eliminates either a set number of points from the calculation, or any values that are more extreme than a certain range (such as three standard deviations from the untrimmed mean). In NumPy, the latter approach is used, and a lower and upper limit can be applied to eliminate values outside this range from being used in the computation of the [trimmed mean][p4].\n",
    "\n",
    "-----\n",
    "\n",
    "[2]: https://en.wikipedia.org/wiki/Medianhttps://en.wikipedia.org/wiki/Mode_(statistics)\n",
    "[p2]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.median.html\n",
    "[3]: https://en.wikipedia.org/wiki/Mode_(statistics)\n",
    "[p3]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "[4]: https://en.wikipedia.org/wiki/Truncated_mean\n",
    "[p4]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.nanmean.html#numpy.nanmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Value = 17.7950\n",
      "Modal Value = 13.4200, occured 3 times.\n",
      "Trimmed Mean = 18.5894 with bounds = 12.50 : 27.50\n"
     ]
    }
   ],
   "source": [
    "print('Median Value = {:6.4f}'.format(np.median(data)))\n",
    "\n",
    "mode = sp.stats.mode(data)\n",
    "print('Modal Value = {0:6.4f}, occured {1} times.'.format(mode[0][0], mode[1][0]))\n",
    "\n",
    "low = 12.5\n",
    "up = 27.5\n",
    "tm = sp.stats.tmean(data, (low, up))\n",
    "print('Trimmed Mean = {0:6.4f} with bounds = {1:4.2f} : {2:4.2f}'.format(tm, low, up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "If our data is stored in a traditional Python data structure, such as a `list`, we can use the built-in `statistics` module to compute these quantities (as well as the lower and upper median), as demonstrated below. This includes an explicit demonstration of calculating the mode for a categorical data type.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 19.7859\n",
      "Median Value = 17.7950\n",
      "Mode Value = 13.4200\n"
     ]
    }
   ],
   "source": [
    "# Use the built-in statistics module for a list\n",
    "import statistics as st\n",
    "\n",
    "#convert NumPy array to a list\n",
    "data_list = list(data)\n",
    "\n",
    "# Compute and display basic statistics\n",
    "print('Mean Value = {:6.4f}'.format(st.mean(data_list)))\n",
    "print('Median Value = {:6.4f}'.format(st.median(data_list)))\n",
    "print('Mode Value = {:6.4f}'.format(st.mode(data_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modal color = brown\n"
     ]
    }
   ],
   "source": [
    "# Categorical data example\n",
    "\n",
    "color_data = ['red', 'blue', 'blue', 'brown', 'brown', 'brown']\n",
    "print('Modal color = {}'.format(st.mode(color_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median (low) Value = 17.7800\n",
      "Median Value = 17.7950\n",
      "Median (high) Value = 17.8100\n"
     ]
    }
   ],
   "source": [
    "# High and low Median example\n",
    "print('Median (low) Value = {:6.4f}'.format(st.median_low(data_list)))\n",
    "print('Median Value = {:6.4f}'.format(st.median(data_list)))\n",
    "print('Median (high) Value = {:6.4f}'.format(st.median_high(data_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the empty **Code** cell below, write and execute code to compute the mean and median for the `total_bill` column, separately for those rows corresponding to _lunch_ and _dinner_. What do these values suggest about the typical lunch or dinner party?\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Measures of Variability (Dispersion)\n",
    "\n",
    "Once we have a measure for the central location of a data set, the next step is to quantify the spread or dispersion of the data around this location. Inherent in this process is the computation of a distance between the data and the central location. Variations in the distance measurement can lead to differences in variability measures, which include the following dispersion measurements:\n",
    "\n",
    "0. Range\n",
    "1. Mean deviation\n",
    "2. Mean absolute deviation\n",
    "3. Variance\n",
    "4. Standard deviation\n",
    "\n",
    "\n",
    "### [Range][1]\n",
    "\n",
    "Of these, the simplest is a range measurement, which is simply the difference between two characteristic values. Often these are the minimum and maximum values in a data set. While simple, this provides little insight into the true spread or dispersion of the data, in particular, about the centrality measure (since the mean might lie anywhere between the maximum and minimum values).\n",
    "\n",
    "### Mean Deviation\n",
    "\n",
    "Each of the remaining four dispersion measurements are made with respect to a location measure, which is typically the mean value. Thus, the mean deviation is typically measured with respect to the mean value, as shown by the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Mean Deviation} = \\frac{\\sum_{i=1}^N (x_i - \\overline{x})}{N}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "While simple, one sums up the deviations from the mean value and divides by the number of points, this measure cancels out high and low measurements, producing a small value measure (which can be zero) for the spread of the data around the mean value. This problem can be overcome by using two different techniques: summing absolute deviations or summing the square of the deviations, which leads to the last three measures.\n",
    "\n",
    "### [Mean Absolute Deviation][2]\n",
    "\n",
    "First, if we sum up the absolute deviations, we generate a sum of intrinsically positive values. This mean absolute deviation is also known as an $l1-$norm, or _Manhattan norm_. This norm finds uses in a number of situations, including in machine learning. The formula for the mean absolute deviation is simple:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Mean Absolute Deviation} = \\frac{\\sum_{i=1}^N |x_i - \\overline{x}|}{N}\n",
    "\\end{equation}\n",
    "\n",
    "### [Variance][3]\n",
    "\n",
    "Second, if we sum up the square of the deviations from the mean, we also end up with a sum of intrinsically positive numbers, which is known as the $l2-$norm, or the _Euclidean norm_. This norm should be familiar, as it is used in the _Pythagorean_ formula, and it is also used in machine learning. \n",
    "\n",
    "The formula for the mean absolute deviation is simple:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Variance} = \\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N}\n",
    "\\end{equation}\n",
    "\n",
    "### [Standard Deviation][4]\n",
    "\n",
    "One concern with the variance, however, is that the units of variance are the square of the original units (for example, length versus length * length). To enable the measure of the variability around the mean to be compared to the mean, we generally will use the standard deviation, which is simply the square root of the variance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Standard Deviation} = \\sigma = \\sqrt{\\left(\\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "### [Coefficient of Variation][5]\n",
    "\n",
    "The value of a dispersion measure can be confusing. On its own, a large or small dispersion is less informative than a dispersion measure combined with a location measure. For example, a dispersion measure of 10 means something different if the mean is 1 versus 100. As a result, the coefficient of variation is sometimes used to encode the relative size of a dispersion to the mean:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Coefficient of Variation} = \\frac{\\textrm{Standard deviation}}{\\textrm{Mean value}} = \\frac{\\sigma}{\\mu}\n",
    "\\end{equation}\n",
    "\n",
    "In the following code cells, we compute these quantities by using built-in functions from the NumPy module, our own computations for the mean deviation and mean absolute deviation, and demonstrate how to compute the variance and standard deviation by using the built-in `statistics` module on data stored in a list.\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Range_(statistics)\n",
    "[2]: https://en.wikipedia.org/wiki/Average_absolute_deviation\n",
    "[3]: https://en.wikipedia.org/wiki/Variance#Sample_variance\n",
    "[4]: https://en.wikipedia.org/wiki/Standard_deviation\n",
    "[5]: https://en.wikipedia.org/wiki/Coefficient_of_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum = 50.8100\n",
      "Minimum = 3.0700\n",
      "Range = 47.7400\n",
      "Peak To Peak (PTP) Range = 47.7400\n",
      "\n",
      "Mean Deviation from Mean = -0.0000\n",
      "Mean Absolute Deviation from Mean = 6.8694\n",
      "\n",
      "Variance = 78.9281\n",
      "Standard Deviation = 8.8842\n",
      "\n",
      "Coefficient of Variation = 0.4490\n"
     ]
    }
   ],
   "source": [
    "# Compute Range quantities\n",
    "print('Maximum = {:6.4f}'.format(np.max(data)))\n",
    "print('Minimum = {:6.4f}'.format(np.min(data)))\n",
    "print('Range = {:6.4f}'.format(np.max(data) - np.min(data)))\n",
    "print('Peak To Peak (PTP) Range = {:6.4f}\\n'.format(np.ptp(data)))\n",
    "\n",
    "# Compute deviations\n",
    "tmp = data - np.mean(data)\n",
    "n = data.shape[0]\n",
    "\n",
    "print('Mean Deviation from Mean = {:6.4f}'.format(np.sum(tmp/n)))\n",
    "print('Mean Absolute Deviation from Mean = {:6.4f}\\n'.format(np.sum(np.absolute(tmp))/n))\n",
    "\n",
    "# Use standard functions for variance\n",
    "print('Variance = {:6.4f}'.format(np.var(data)))\n",
    "print('Standard Deviation = {:6.4f}\\n'.format(np.std(data)))\n",
    "\n",
    "# Coefficient of Variation\n",
    "print('Coefficient of Variation = {:6.4f}'.format(np.std(data)/np.mean(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance = 79.2529\n",
      "Standard Deviation = 8.9024\n"
     ]
    }
   ],
   "source": [
    "# Use Python's built-in statistics module\n",
    "# Works with list data\n",
    "\n",
    "print('Variance = {:6.4f}'.format(st.variance(data_list)))\n",
    "print('Standard Deviation = {:6.4f}'.format(st.stdev(data_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the empty **Code** cell below, write and execute code to compute the mean absolute deviation and standard deviation for the `total_bill` column, separately for those rows corresponding to _lunch_ and _dinner_. What do these values suggest about the typical lunch or dinner party? Also, compare the coefficient of variation for the same data. Do these values also make sense?\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Distribution\n",
    "\n",
    "While useful, measures of location and dispersion are only two numbers. Thus, other techniques exist for analytically quantifying a distribution of data. Fundamentally, these techniques all rely on sorting the data and finding the value  below which a certain percentage of data lie. In this manner, one _percentile_ would correspond to one percent of the data, while the median would correspond to fifty percent of the data. Formally, the following pre-defined measures are commonly used:\n",
    "\n",
    "4. Percentiles: Divides the data into one percent chunks, the value indicates which percentile is of interest\n",
    "5. Deciles: Divides the data into ten chunks, each containing 10% of the data\n",
    "6. Quantiles: Divides the data into five chunks, each containing 20% of the data\n",
    "7. Quartiles: Divides the data into four chunks, each containing 25% of the data\n",
    "\n",
    "With NumPy we just use `percentile` function for all of these and specify the appropriate value for the percentile. Thus for the second decile, we specify `np.percentile(data, 20)`. The following code cell demonstrates some of the more common values.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median (via percentile) = 17.7950\n",
      "First Quartile = 13.3475\n",
      "Second Quartile = 17.7950\n",
      "Third Quartile = 24.1275\n",
      "Fourth Quartile = 50.8100\n",
      "Quartile Range = 10.7800\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate percentile with the Median\n",
    "\n",
    "print('Median (via percentile) = {:6.4f}'.format(np.percentile(data, 50)))\n",
    "\n",
    "# Compute quartiles\n",
    "print('First Quartile = {:6.4f}'.format(np.percentile(data, 25)))\n",
    "print('Second Quartile = {:6.4f}'.format(np.percentile(data, 50)))\n",
    "print('Third Quartile = {:6.4f}'.format(np.percentile(data, 75)))\n",
    "print('Fourth Quartile = {:6.4f}'.format(np.percentile(data, 100)))\n",
    "\n",
    "# Interquartile range is sometimes used as a dispersion measure\n",
    "print('Quartile Range = {:6.4f}'.format(np.percentile(data, 75) - np.percentile(data, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Quantile = 12.6360\n",
      "Second Quantile = 16.2220\n",
      "Third Quantile = 19.8180\n",
      "Fourth Quantile = 26.0980\n"
     ]
    }
   ],
   "source": [
    "# Compute quantiles\n",
    "print('First Quantile = {:6.4f}'.format(np.percentile(data, 20)))\n",
    "print('Second Quantile = {:6.4f}'.format(np.percentile(data, 40)))\n",
    "print('Third Quantile = {:6.4f}'.format(np.percentile(data, 60)))\n",
    "print('Fourth Quantile = {:6.4f}'.format(np.percentile(data, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the empty **Code** cell below, write and execute code to compute the interquartile range for the `total_bill` column, separately for those rows corresponding to _lunch_ and _dinner_. What do these values suggest about the typical lunch or dinner party? Also, compute all quantiles for these same data, and compare the spread of the data in both data sets.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Weighted Statistics\n",
    "\n",
    "Often the data we analyze should be treated differently. For example, if the data have associated errors, some data will have smaller errors and thus be more precise measurements. Likewise, in some situations, we will have more data of one class (such as _lunch_ receipts) when compared to other classes (like _dinner_ receipts). As a result, we will want to weight the data appropriately to handle the additional information or to emphasize special cases of the analysis. \n",
    "\n",
    "In these cases, given a set of data ($x_i$) with associated weights ($w_i$), we can compute weighted measures for the mean ($\\mu_w$) and standard deviation ($\\sigma_w$). In both cases, we simply weight the values appropriately and correct our normalization for the weights.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_w = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{i=1}^N w_i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_w = \\sqrt{\\frac{\\sum_{i=1}^N w_i (x_i - \\mu_w)^2}{\\sum_{i=1}^N w_i}}\n",
    "\\end{equation}\n",
    "\n",
    "These calculations are demonstrated in Python in the following code cell, where we generate random weights. The NumPy module does provide a function to compute a weighted mean (`average`) but does not yet include a function for computing the weighted standard deviation; thus, we compute this quantity according to the previous formula and display the unweighted and weighted versions of these quantities.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 19.7859\n",
      "Weighted Mean = 19.8027\n",
      "\n",
      "Standard Deviation = 8.8842\n",
      "Weighted Standard Deviation = 8.6036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# generate random weights\n",
    "w = np.random.uniform(size=data.shape)\n",
    "\n",
    "# Compute weighted mean\n",
    "wm = np.average(data, weights=w)\n",
    "\n",
    "# Compute weighted standard deviation\n",
    "wstd = math.sqrt(np.average((data - wm)**2, weights = w))\n",
    "\n",
    "# Compute and display normal & weighted statistics\n",
    "print('Mean = {:6.4f}'.format(np.sum(np.mean(data))))\n",
    "print('Weighted Mean = {:6.4f}\\n'.format(wm))\n",
    "\n",
    "print('Standard Deviation = {:6.4f}'.format(np.std(data)))\n",
    "print('Weighted Standard Deviation = {:6.4f}\\n'.format(wstd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Measures of Shape\n",
    "\n",
    "In contrast to measures of the distribution, such as the _percentile_ or _quantile_, there are several other pre-defined quantities that provide insight into the _shape_ of a data set, especially in relation to the mean and standard deviation. The two most prominent such measures are the [skewness][ws] and the [kurtosis][wk]. The skewness measures the lack of symmetry with respect to the mean value. Values near zero indicate symmetric distributions, while larger values indicate increasing asymmetry. Kurtosis, on the other hand, measures the spread (or how wide the tails are) of a distribution relative to the Normal distribution. Small values of Kurtosis indicate data that are highly concentrated around the mean value, while large values of the kurtosis indicate data that are considerably more different than the mean value.\n",
    "\n",
    "These quantities can easily be calculated in Python by using the appropriate functions from the Scipy module, as shown in the following code blocks.\n",
    "\n",
    "-----\n",
    "\n",
    "[ws]: https://en.wikipedia.org/wiki/Skewness\n",
    "[wk]: https://en.wikipedia.org/wiki/Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness = 1.1262\n"
     ]
    }
   ],
   "source": [
    "skew = sp.stats.skew(data)\n",
    "print('Skewness = {0:6.4f}'.format(skew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis = 1.1692\n"
     ]
    }
   ],
   "source": [
    "kurt = sp.stats.kurtosis(data)\n",
    "print('Kurtosis = {0:6.4f}'.format(kurt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Population or Sample\n",
    "\n",
    "To this point, we have focused on describing a data set as if it stood alone. For example, you might have access to the full financial statements of a company or information about every transaction that took place in a given market. In some cases, however, you may only have a subset of this full information. This may arise when the original data is very large and you simply want to explore a subset of the full data, or perhaps you were simply given a subset in order to ascertain any issues of problems. \n",
    "\n",
    "Formally, this division can be described as being given either the full population or simply a sample. Traditional statistics, being developed many years ago when data sets were small and all calculations were done with pencil and paper, focused on using samples to make predictions (or estimates) of the full population. In practice, this introduces several new terms and small changes in calculating descriptive statistics. First, the originating data set is often called the _parent population_. Second, the process of selecting data from the parent population is known as _sampling_. This relationship is encoded in the following figure:\n",
    "\n",
    "![Population to Sample](images/p2s.png)\n",
    "\n",
    "When using a _sample_ to describe the _parent population_, we must account for the fact that we are using limited information to describe something (potentially) much larger. Thus, when we use a sample to estimate the _mean_ value of the parent population, we have reduced the information content of our sample. This means that when we use this _estimated mean_ value to compute the _estimated standard deviation_, we actually compute a less precise estimate. This is formalized by a concept known as _degree of freedom_, which is given by the number of data points in the sample, N, which is equal to the length of the data set in Python.\n",
    "\n",
    "Thus, we can estimate the mean (or average) value of a parent population by computing the mean value of the sample ($\\overline{x}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{x} = \\frac{\\sum_{i = 1}^N x_i}{N}\n",
    "\\end{equation}\n",
    "\n",
    "Since we have now used the sample to estimate one descriptive statistic, the arithmetic mean of the parent population, we have reduced the _degree of freedom_ in computing the variance or standard deviation of the parent population from the sample. Thus, we must divide the standard deviation estimate by $ N - 1 $, not $N$, as shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{var} = \\frac{\\sum_{i = 1}^N (x_i - \\overline{x})^2}{N - 1}\n",
    "\\end{equation}\n",
    "\n",
    "We often use the sample standard deviation ($s$), since this has the same units as the mean value, which is the square root of the variance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{s} = \\sqrt{\\frac{\\sum_{i = 1}^N (x_i - \\overline{x})^2}{N - 1}}\n",
    "\\end{equation}\n",
    "\n",
    "To calculate this by using Python, we can simply pass in _delta degrees of freedom_ parameter to the `numpy.std` method:\n",
    "\n",
    "```python\n",
    "np.std(data, ddof=1)\n",
    "```\n",
    "\n",
    "Another statistic that is often of interest is the precision with which we can measure the mean value. Intuitively, as the number of points in our sample increases, the precision should increase (and the standard deviation decrease). This is formulated by the sample _standard error_, or SE, which measures the standard error in quantifying a statistic. Thus, if we want to know the precision in our measurement of the mean, we can compute the _sample standard error of the mean_. Formally, this value is computed by dividing the sample standard deviation by the square root of the number of data points in the sample:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{SE} = \\frac{s}{\\sqrt{N}}\n",
    "\\end{equation}\n",
    "\n",
    "Note that for large data sets, the difference in dividing by $N$ or $N - 1$ is minimal and we thus can often ignore the difference in practice.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation = 8.8842\n",
      "Sample Standard Deviation = 8.9024\n"
     ]
    }
   ],
   "source": [
    "# Compare population and sample standard deviations\n",
    "print('Standard Deviation = {:6.4f}'.format(np.std(data)))\n",
    "print('Sample Standard Deviation = {:6.4f}'.format(np.std(data, ddof=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. [NumPy statistics][1] documentation\n",
    "2. The [Python Statistics module][2]\n",
    "3. Blog article by Randal Olsen on [statistical analysis][4] with Pandas\n",
    "4. The [exploratory data analysis chapter][ts2c] from the book _Think Stats 2e_ by Allen Downey\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: https://docs.scipy.org/doc/numpy/reference/routines.statistics.html\n",
    "[2]: https://docs.python.org/3/library/statistics.html\n",
    "[4]: http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/\n",
    "[ts2c]: http://greenteapress.com/thinkstats2/html/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
